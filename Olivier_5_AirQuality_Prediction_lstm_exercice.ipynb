{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerfs21/notebooks/blob/main/Olivier_5_AirQuality_Prediction_lstm_exercice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7eF5XdWbonX"
      },
      "source": [
        "## Deep learning: Time Series forecasting\n",
        "**Author:** [Dr. Habiboulaye Amadou-Boubacar](https://www.linkedin.com/in/habiboulaye-amadou-boubacar-8b153710)\n",
        "\n",
        "This notebook takes inspiration from works of Jason Brownlee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rd1dHwUPr6u"
      },
      "source": [
        "## Tutorial on Deep learning: Time Series forecasting\n",
        "### Recurent Neural Networks: Develop an LSTM (Long Short-Term Memory)\n",
        "* Air Pollution data\n",
        "* Baseline model\n",
        "* Vanilla LSTM model\n",
        "* LSTM with scaled data\n",
        "* StackedLSTM\n",
        "* LTSM-CNN (optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SLUlWGNuei8"
      },
      "source": [
        "### Populating namespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMNCCjSHudcb"
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pylab as plt\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3jgFUNdi6d_"
      },
      "source": [
        "### Air Pollution dataset ([Beijing PM2.5 Data Set](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv))\n",
        "The dataset is collected from the US embassy in Beijing, China. It reports the Air quality and the weather each hour for five(5) years.\n",
        "The data including the pollutant (PM2.5 concentration) to forecast is described with varibles listed below:\n",
        "* No: row number\n",
        "* year: year of data in this row\n",
        "* month: month of data in this row\n",
        "* day: day of data in this row\n",
        "* hour: hour of data in this row\n",
        "* pm2.5: PM2.5 concentration\n",
        "* DEWP: Dew Point\n",
        "* TEMP: Temperature\n",
        "* PRES: Pressure\n",
        "* cbwd: Combined wind direction\n",
        "* Iws: Cumulated wind speed\n",
        "* Is: Cumulated hours of snow\n",
        "* Ir: Cumulated hours of rain\n",
        "\n",
        "The Goal is to forecast the pollution at the next hour given history of pollution and weather condition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHFvpqWCmH8p"
      },
      "source": [
        "#### Download [Beijing PM2.5 Data Set](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fYQqbuCieTS"
      },
      "source": [
        "!rm pollution.csv\n",
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFOkHZeEoM7A"
      },
      "source": [
        "<font color='red'>\n",
        "<b>EXERCICES</b>: Replace the <b>FILL_IN</b> pattern with the correct codes then execute the cell\n",
        " </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdm_7QuFny0z"
      },
      "source": [
        "data = pd.read_csv('pollution.csv')\n",
        "print(data.shape)\n",
        "# Display the first (5) rows of the dataframe\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnF2K1OJo8pI"
      },
      "source": [
        "# Check NAs (missing values) for all the columns\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUva8e3lvzNk"
      },
      "source": [
        "# Check date types\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGeVhowH1tNz"
      },
      "source": [
        "#Check variable cbwd value occurences using value_counts\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx2B5qCSrfkm"
      },
      "source": [
        "#### Reload, preprocess and visualize data\n",
        "<font color='red'>\n",
        "EXERCICE: Replace the <FILL_IN> with the correct codes to complete the code\n",
        " </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLtrI0pyqven"
      },
      "source": [
        "# Load & format date\n",
        "data = pd.read_csv('pollution.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=lambda d: datetime.strptime(d, '%Y %m %d %H'))\n",
        "# Drop No(variable) column\n",
        "<FILL_IN>\n",
        "# Fill all the NA with value  0\n",
        "<FILL_IN>\n",
        "# label encodeing of cbwd (wind direction feature): categories to numerics\n",
        "<FILL_IN> # Create an instance of LabelEncoder\n",
        "<FILL_IN> # use the fit__transform function to encode the data\n",
        "# rename columns\n",
        "data.rename(columns = {\"pm2.5\":\"pollution\",\"cbwd\":\"WINDdir\",\"Iws\":\"WINDsped\",\t\"Is\":\"SNOW\",\t\"Ir\":\"RAIN\"}, inplace=True)\n",
        "data.index.name = 'datetime'\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv8SMGBrr5ml"
      },
      "source": [
        "plt.figure(figsize=(15,20))\n",
        "list_vars = data.columns #(\"pollution\",\t\"DEWP\",\t\"TEMP\",\t\"PRES\",\t\"WINDsped\",\t\"SNOW\",\t\"RAIN\")\n",
        "for i, var in enumerate(list_vars):\n",
        "  plt.subplot(data.shape[1], 1, i+1)\n",
        "  plt.plot(data[var], color='green')\n",
        "  plt.title(var,y=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlUmR_TBJjJZ"
      },
      "source": [
        "data.pollution.hist(bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtqEvidA3KNj"
      },
      "source": [
        "## Time serie forecasting as supervised learning problem (Regression setting)\n",
        "* predict the pollution at the next hour (t) given the pollution and weather conditions at the prior time step\n",
        "\n",
        "<font color='red'>\n",
        "EXERCICE: Replace the <FILL_IN> with the correct codes to complete the code\n",
        " </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ykuptCRq19"
      },
      "source": [
        "#### Prepare the pollution dataset for LSTM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PUocM49xJXe"
      },
      "source": [
        "def history_and_horizon_sequencing(df, n_history, n_horizon, target=None):\n",
        "  # History: look-back sequences (t-n, ... t-1)\n",
        "  stack_history = []\n",
        "  for i in range(n_history,0,-1):\n",
        "    df_i = df.shift(i)\n",
        "    df_i.columns = [f'{col}_t-{i}' for col in df_i.columns]\n",
        "    stack_history =  stack_history + [df_i]\n",
        "  # target dataframe\n",
        "  if target is None: df_target = df\n",
        "  else: df_target = df[target].to_frame()\n",
        "  # Horizon: step-ahead sequences (t+1, ... t+n)\n",
        "  stack_horizon = []\n",
        "  for j in range(n_horizon,0,-1):\n",
        "    df_j = df_target.shift(j)\n",
        "    df_j.columns = [f'{col}_t+{j}' for col in df_j.columns]\n",
        "    stack_horizon = [df_j] + stack_horizon\n",
        "  # Present: t\n",
        "  df_t = df.copy()\n",
        "  df_t.columns = [f'{col}_t' for col in df_t.columns]\n",
        "  # return the concatenated data frame: past+present+future\n",
        "  return pd.concat(stack_history+[df_t]+stack_horizon, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb8JQJ_DzinY"
      },
      "source": [
        "data_Xy = history_and_horizon_sequencing(data, 3, 1, target='pollution')\n",
        "print(data_Xy.shape)\n",
        "data_Xy.dropna(inplace=True)\n",
        "print(data_Xy.shape)\n",
        "data_Xy.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIfyF4dCXRDc"
      },
      "source": [
        "#### Baseline v0: persistance modeling\n",
        "Use observation from the present time step (t) to predict the observation at the next time step (t+1).   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSeutCqv_4r_"
      },
      "source": [
        "def baseline_persistance(serie_t):\n",
        "  # forecast: predict t+1 with value of t\n",
        "  <FILL_IN>\n",
        "\n",
        "def compute_performance(test, forecast, start_t=400, end_t=500):\n",
        "  <FILL_IN> # Compute root mean squared error between test and forecast\n",
        "  print('RMSE: %.3f' % rmse)\n",
        "  plt.figure(figsize=(15,5))\n",
        "  plt.plot(test[start_t:end_t], color='b')\n",
        "  plt.plot(forecast[start_t:end_t], color='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ELTaf5M7Y6N"
      },
      "source": [
        "test_y, persistance_yhat = data_Xy['pollution_t'][1:], baseline_persistance(data_Xy['pollution_t'])[1:]\n",
        "compute_performance(test_y, persistance_yhat, start_t=365*24, end_t=365*24+200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS26_Zg0kG-8"
      },
      "source": [
        "#### LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfle5vCskQhF"
      },
      "source": [
        "#### Split data into train and test sets\n",
        "Build train and test dataset for training the model on the first one(1) year and prediction of the four(4) lastest years"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj0EBPsdkFwx"
      },
      "source": [
        "def split_train_test(values, n_train_hours = 365*24):\n",
        "  # Split the data into train (use values before n_train_hours) and test sets (use values after n_train_hours)\n",
        "  <FILL_IN>\n",
        "  <FILL_IN>\n",
        "  # split into input and outputs\n",
        "  train_X, train_y = train[:, :-1], train[:, -1]\n",
        "  test_X, test_y = test[:, :-1], test[:, -1]\n",
        "  # reshape input to be 3D [samples, timesteps, features] required for LSTM\n",
        "  train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "  test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "  print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "  return train_X, train_y, test_X, test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd3IGzyTjrtn"
      },
      "source": [
        "train_X, train_y, test_X, test_y = split_train_test(data_Xy.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFpDnyWlp_Pu"
      },
      "source": [
        "#### Build an LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnNC9ZfejsDv"
      },
      "source": [
        "def build_lstm_model(input_shape, nb_neurons = 50):\n",
        "  '''\n",
        "  train and test: input shape 1 time step with 8 features\n",
        "  LSTM:\n",
        "   * nb_neurons: hidden state\n",
        "   * 1 neuron for output layer for prediction.\n",
        "  '''\n",
        "  model = Sequential()\n",
        "  # Add an LSTM layer with nb_neurons and input_shape=input_shape\n",
        "  <FILL_IN>\n",
        "  # Add a Dense layer with one output neuron\n",
        "  <FILL_IN>\n",
        "  # model compile\n",
        "  model.compile(loss='mae', optimizer='adam')\n",
        "  # Train the model\n",
        "  return model\n",
        "\n",
        "def train_model(model, train_X, train_y, test_X, test_y, epochs=500, batch_size=72, verbose=0):\n",
        "  '''\n",
        "  Generic function to train model\n",
        "  '''\n",
        "  history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=verbose, shuffle=False)\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frTEtstBEUg3"
      },
      "source": [
        "# Build and train the model\n",
        "input_shape=(train_X.shape[1], train_X.shape[2])\n",
        "# Build and train the model using the build_lstm_model (new instance lstm_model)\n",
        "<FILL_IN>\n",
        "# Train the new model using train_model method with params lstm_model, train_X, train_y, test_X, test_y, epochs=100, batch_size=72, verbose=0\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr9Pz8oFoCWE"
      },
      "source": [
        "# make the prediction of test_X (store in lstm_yhat)\n",
        "lstm_yhat = lstm_model.predict(test_X)\n",
        "# compute performance\n",
        "compute_performance(test_y, lstm_yhat, start_t=365*24, end_t=365*24+200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cgpHBBE9Wj"
      },
      "source": [
        "#### LSTM trained with scaling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeXIl8ix3yc5"
      },
      "source": [
        "# Normalize features\n",
        "values = data_Xy.values.astype('float32')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_values = scaler.fit_transform(values)\n",
        "# Create train/tests datasets from scaled_data\n",
        "scaled_train_X, scaled_train_y, scaled_test_X, scaled_test_y = split_train_test(scaled_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VbDMNnBGk1L"
      },
      "source": [
        "# Build and train the model using the build_lstm_model (new instance name lstm_model_scaled)\n",
        "<FILL_IN>\n",
        "# Train the new model using train_model method as previousely using the new scaled data\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ6E5G1pPi2p"
      },
      "source": [
        "# invert scaling for forecast\n",
        "def invert_scaling(scaled_X, scaled_yhat):\n",
        "  scaled_tX = scaled_test_X.reshape((scaled_X.shape[0], scaled_X.shape[2]))\n",
        "  Xyat = concatenate((scaled_tX, scaled_yhat.reshape(-1,1)), axis=1)\n",
        "  inv_Xyat = scaler.inverse_transform(Xyat)\n",
        "  return inv_Xyat[:,-1]\n",
        "\n",
        "# make a prediction\n",
        "lstm_scaled_yhat = lstm_model_scaled.predict(scaled_test_X)\n",
        "# compute performance\n",
        "inv_yhat = invert_scaling(scaled_test_X, lstm_scaled_yhat)\n",
        "compute_performance(test_y, inv_yhat, start_t=365*24, end_t=365*24+200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDa6VLMByKKH"
      },
      "source": [
        "#### Build an Stacked LSTM model\n",
        "\n",
        "Stacked LSTM model refers to stacking multiple hidden LSTM layers one on top of another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoX6PFukyIwI"
      },
      "source": [
        "def build_stacked_lstm_model(input_shape, nb_neurons = 50):\n",
        "  model = Sequential()\n",
        "  # Add an LSTM layer with nb_neurons and input_shape=input_shape and return_sequences=True (required sequences needed for next LSTM Bloc)\n",
        "  <FILL_IN>\n",
        "  # Add an additional LSTM layer with nb_neurons and 'relu' activation function\n",
        "  <FILL_IN>\n",
        "  # Add a Dense layer with one output neuron\n",
        "  <FILL_IN>\n",
        "  # Train the model\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOspJ8ljZMc1"
      },
      "source": [
        "input_shape = (train_X.shape[1], train_X.shape[2])\n",
        "# Build the new stacked_lstm_model\n",
        "<FILL_IN>\n",
        "# Train the model stacked_lstm_model\n",
        "<FILL_IN>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4g65nJpYAYa"
      },
      "source": [
        "# make a prediction with stacked_lstm_model using scaled_test_X (output in variable scaled_stacked_lstm_yhat)\n",
        "<FILL_IN>\n",
        "# compute performance\n",
        "inv_yhat = invert_scaling(scaled_test_X, scaled_stacked_lstm_yhat)\n",
        "compute_performance(test_y, inv_yhat, start_t=365*24, end_t=365*24+200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsfoEbgsg7g2"
      },
      "source": [
        "## Assignment: Work to improve the performance - try different strategies:\n",
        "* Hyperparameters tuning\n",
        "* Test other models:\n",
        " - Eg. 1D-ConvNet LSTM, Bidirectional"
      ]
    }
  ]
}