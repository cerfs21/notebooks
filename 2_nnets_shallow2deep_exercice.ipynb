{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerfs21/notebooks/blob/main/2_nnets_shallow2deep_exercice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Practices and illustrations to introduces Neural Networks"
      ],
      "metadata": {
        "id": "xdC24v3yghB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Dr. Habiboulaye {@gmail.com}\n",
        "\n",
        "This hand-on tutorial will conver:\n",
        "\n",
        "- Implementation of 1 Neuron\n",
        "- Implementation of 2 layers NNet with feedforward function\n",
        "- How to go from Shallow to Deep Neural Networks"
      ],
      "metadata": {
        "id": "t2R7ULqigBfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Neuron: the basic unit of a Neural Network"
      ],
      "metadata": {
        "id": "Yr-UyY_0wNj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neuron is the basic unit of a neural network\n",
        "It compute a weighted sum of the inputs and apply an activation function to the result to produce output. feed the input(s) forward through the neurons in the network to get the output(s)"
      ],
      "metadata": {
        "id": "-r56toHDvXK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src='https://github.com/habiboulaye/ml-learn/blob/main/Neuron.png?raw=true' align='center' width=400>\n",
        "\n"
      ],
      "metadata": {
        "id": "DSNgHlwlwQVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$Y = g(W.X+b)$$\n",
        "$$g(x) = \\frac{1}{(1 + e^{(-x)})}$$\n"
      ],
      "metadata": {
        "id": "Wyxp-T6_x_45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <I>Coding a Neuron </I>"
      ],
      "metadata": {
        "id": "Qdu_ioh1wyuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function: sigmoid (a special case of logistic function)\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "# Define our Neuron as an object oriented class\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    '''\n",
        "      Initilise the parameters\n",
        "    '''\n",
        "    self.w = weights\n",
        "    self.b = bias\n",
        "\n",
        "  def forward_prop(self, x):\n",
        "    '''\n",
        "      1. Compute a weighted sum of inputs augmented with bias using numpy dot method\n",
        "      2. Apply activation function to compute output y and return\n",
        "    '''\n",
        "    z=np.dot(self.w,x) + self.b\n",
        "    y=sigmoid(z)\n",
        "    return(y)\n",
        "\n",
        "  def backward_prop():\n",
        "    \"\"\"\n",
        "      NOT Implemented: GD to optimize params\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "0g2bRbp8wyIX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iM3qsZQVf8c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2766c557-f7c2-46d7-d93a-49be9736d1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9168273035060777\n",
            "Well done! Congrat !\n"
          ]
        }
      ],
      "source": [
        "weights = np.array([0.3, 0.2])\n",
        "bias = 2\n",
        "\n",
        "aNeuron = Neuron(weights, bias)\n",
        "\n",
        "inputs = np.array([-2, 5])\n",
        "#output?\n",
        "output = aNeuron.forward_prop(inputs)\n",
        "print(output)\n",
        "assert output == 0.9168273035060777, \"Error Debug - Try again ...\"\n",
        "print('Well done! Congrat !')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Connecting Neurons to create a Network[texte du lien]\n",
        "A Neural network is just a bunch of neurons connected together. It feeds the input(s) forward through the neurons in the network to get the output(s) at the end.\n",
        "\n",
        "* One Input layer receiving inputs\n",
        "* One (or many) hidden layer for modeling more or less complex function\n",
        "* One Output layer exposing the outputs"
      ],
      "metadata": {
        "id": "6IpaAY3e-6AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src='https://github.com/habiboulaye/ml-learn/blob/main/2layers_NNets.png?raw=true' align='center' width=400 >"
      ],
      "metadata": {
        "id": "NTb_I1eAcU-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$X^{[1]} = g(W^{[1]}.X^{[0]}+b^{[1]})$$\n",
        "$$Y = g(W^{[2]}.X^{[1]}+b^{[2]})$$\n"
      ],
      "metadata": {
        "id": "7gLTYaukcnTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <I>Coding a 2-layer Neural Network </I>"
      ],
      "metadata": {
        "id": "AbgUA3SaBGNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 layers neural networks\n",
        "# Input layers with 2 neurons, Hidden layer with 2 neurons and output layer with one neuron\n",
        "class TwoLayersNeuralNetwork:\n",
        "  def __init__(self, parameters):\n",
        "    '''\n",
        "       Instanciate the hidden an output neuron (use the class aNeuron implemented above) with initial parameters\n",
        "    '''\n",
        "    params = {'h1n1_weights': W, 'h1n2_weights': W, 'outn_weights': W, 'h1n1_bias': b, 'h1n2_bias': b, 'outn_bias': b}\n",
        "\n",
        "    self.hiddenNeuron1 = Neuron(parameters['h1n1_weights'], parameters['h1n1_bias'])\n",
        "    self.hiddenNeuron2 = Neuron(parameters['h1n2_weights'], parameters['h1n2_bias'])\n",
        "    self.outputNeuron  = Neuron(parameters['outn_weights'], parameters['outn_bias'])\n",
        "\n",
        "  def forward_prop(self, X):\n",
        "    # Compute outputs of hidden neurons\n",
        "    h1n1_out = self.hiddenNeuron1.forward_prop(X)\n",
        "    h1n2_out = self.hiddenNeuron1.forward_prop(X)\n",
        "    # Create a vector using previous outputs for the next steps\n",
        "    h1_out = np.array([h1n1_out, h1n2_out])\n",
        "    # Compute the output of the network\n",
        "    output = self.outputNeuron.forward_prop(h1_out)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backword_prop(self, X):\n",
        "    '''\n",
        "      Gradient descent: Rule chain\n",
        "    '''\n",
        "    pass"
      ],
      "metadata": {
        "id": "EmIAQ3Dq42DB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial params\n",
        "W, b = [0.3,0.1], 5\n",
        "params = {'h1n1_weights': W, 'h1n2_weights': W, 'outn_weights': W, 'h1n1_bias': b, 'h1n2_bias': b, 'outn_bias': b}\n",
        "\n",
        "#Instanciate the network\n",
        "objNNet = TwoLayersNeuralNetwork(params)\n",
        "\n",
        "inputs = np.array([-2, 5])\n",
        "output = objNNet.forward_prop(inputs)\n",
        "print(output)\n",
        "assert output == 0.9954904734490755, \"Error  - Try again ...\"\n",
        "print('Well done! Congrats !')"
      ],
      "metadata": {
        "id": "MKF3FEF5G-Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cb4785-f02a-4771-e549-97e73961eb62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9954904734490755\n",
            "Well done! Congrats !\n"
          ]
        }
      ]
    }
  ]
}